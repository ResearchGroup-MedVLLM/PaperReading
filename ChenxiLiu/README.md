# 相关文献列表

| 序号 | 文章标题（链接） | 备注 |
| ---- | -------- | ---- |
| 1 | [Whether you can locate or not Interactive Referring Expression Generation](https://dl.acm.org/doi/abs/10.1145/3581783.3612214) | 用rec增强了reg |
| 2 | [Alpha-CLIP: A CLIP Model Focusing on Wherever You Want](https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Alpha-CLIP_A_CLIP_Model_Focusing_on_Wherever_You_Want_CVPR_2024_paper.html) | 加了一个alpha通道增强reg |
| 3 | [MiniGPT-v2: Large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478) | 基于kosmos2，适用于各种视觉任务 |
| 4 | [Kosmos-2: Grounding multimodal large language models to the world](https://arxiv.org/abs/2306.14824) | - |
| 5 | [Prompting Vision-Language Models For Aspect-Controlled Generation of Referring Expressions](https://aclanthology.org/2024.findings-naacl.178/) | 为同一目标区域生成多个表达式 |
| 6 | [LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task](https://arxiv.org/abs/2407.12064) | 基于miniGPT2，应用于医学领域，使用了两个视觉编码器 |
| 7 | [Intrinsic Task-based Evaluation for Referring Expression Generation](https://arxiv.org/abs/2402.07432) | 更好的评估方式，基于人工，应该是没法复刻 |
| 7.1 有一点reg评估 | [Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception](https://arxiv.org/abs/2403.02969) | 这篇文章主要还是分割 |
| 8 | [Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring](https://arxiv.org/abs/2403.09333) | reg得到了sota性能，优于kosmos |
| 9 | [Resilience through Scene Context in Visual Referring Expression Generation](https://arxiv.org/abs/2404.12289) | 上下文信息作为一种资源，使REG模型更具弹性 |


